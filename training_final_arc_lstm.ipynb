{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63147d18-3777-4f08-b5b0-202508a74b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribusi Severity sebelum SMOTE: [38232  4580  9206 26227]\n",
      "Distribusi Severity setelah SMOTE: [38232 38232 38232 38232]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load cleaned dataset\n",
    "file_path = \"Cleaned_Merged_data_alarm.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Pisahkan fitur dan target\n",
    "X = df.drop(columns=[\"Severity\"])  # Fitur\n",
    "y = df[\"Severity\"]  # Target\n",
    "\n",
    "# Normalisasi fitur numerik\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Splitting Data (80% Train, 20% Test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Mengatasi Ketidakseimbangan Kelas dengan SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Tampilkan distribusi kelas setelah SMOTE\n",
    "severity_distribution_resampled = np.bincount(y_train_resampled)\n",
    "severity_distribution_original = np.bincount(y_train)\n",
    "\n",
    "print(\"Distribusi Severity sebelum SMOTE:\", severity_distribution_original)\n",
    "print(\"Distribusi Severity setelah SMOTE:\", severity_distribution_resampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3507388-980a-4500-b2fd-679caf03b4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset train-test telah berhasil disimpan ulang.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load cleaned dataset\n",
    "file_path = \"Cleaned_Merged_data_alarm.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Pisahkan fitur dan target\n",
    "X = df.drop(columns=[\"Severity\"])\n",
    "y = df[\"Severity\"]\n",
    "\n",
    "# Normalisasi fitur numerik\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Splitting Data (80% Train, 20% Test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Mengatasi Ketidakseimbangan Kelas dengan SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Simpan dataset yang telah diproses\n",
    "np.save(\"X_train.npy\", X_train_resampled)\n",
    "np.save(\"y_train.npy\", y_train_resampled)\n",
    "np.save(\"X_test.npy\", X_test)\n",
    "np.save(\"y_test.npy\", y_test)\n",
    "\n",
    "print(\"Dataset train-test telah berhasil disimpan ulang.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b845f2e-de6c-4ddc-84cc-48620f8e92c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 12:56:23.704684: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "/home/ubuntu/ryh_training/venv/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4779/4779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - accuracy: 0.8011 - loss: 0.5084 - val_accuracy: 0.8890 - val_loss: 0.2974\n",
      "Epoch 2/50\n",
      "\u001b[1m4779/4779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8862 - loss: 0.3084 - val_accuracy: 0.9057 - val_loss: 0.2659\n",
      "Epoch 3/50\n",
      "\u001b[1m4779/4779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8938 - loss: 0.2888 - val_accuracy: 0.9074 - val_loss: 0.2676\n",
      "Epoch 4/50\n",
      "\u001b[1m4779/4779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8984 - loss: 0.2741 - val_accuracy: 0.8924 - val_loss: 0.2868\n",
      "Epoch 5/50\n",
      "\u001b[1m4779/4779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9025 - loss: 0.2643 - val_accuracy: 0.9060 - val_loss: 0.2678\n",
      "Epoch 6/50\n",
      "\u001b[1m4779/4779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9063 - loss: 0.2543 - val_accuracy: 0.9090 - val_loss: 0.2482\n",
      "Epoch 7/50\n",
      "\u001b[1m4779/4779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.9069 - loss: 0.2542 - val_accuracy: 0.9160 - val_loss: 0.2502\n",
      "Epoch 8/50\n",
      "\u001b[1m4779/4779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9096 - loss: 0.2472 - val_accuracy: 0.9013 - val_loss: 0.2515\n",
      "Epoch 9/50\n",
      "\u001b[1m4779/4779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9106 - loss: 0.2455 - val_accuracy: 0.9143 - val_loss: 0.2366\n",
      "Epoch 10/50\n",
      "\u001b[1m4779/4779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9118 - loss: 0.2382 - val_accuracy: 0.9138 - val_loss: 0.2343\n",
      "Epoch 11/50\n",
      "\u001b[1m4779/4779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9121 - loss: 0.2377 - val_accuracy: 0.9053 - val_loss: 0.2608\n",
      "Epoch 12/50\n",
      "\u001b[1m4779/4779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9137 - loss: 0.2321 - val_accuracy: 0.9059 - val_loss: 0.2537\n",
      "Epoch 13/50\n",
      "\u001b[1m4779/4779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9146 - loss: 0.2315 - val_accuracy: 0.9104 - val_loss: 0.2407\n",
      "Epoch 14/50\n",
      "\u001b[1m4779/4779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9164 - loss: 0.2273 - val_accuracy: 0.9135 - val_loss: 0.2382\n",
      "Epoch 15/50\n",
      "\u001b[1m4779/4779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9174 - loss: 0.2246 - val_accuracy: 0.9147 - val_loss: 0.2324\n",
      "Epoch 16/50\n",
      "\u001b[1m4779/4779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9180 - loss: 0.2236 - val_accuracy: 0.9194 - val_loss: 0.2263\n",
      "Epoch 17/50\n",
      "\u001b[1m4779/4779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9187 - loss: 0.2228 - val_accuracy: 0.9197 - val_loss: 0.2473\n",
      "Epoch 18/50\n",
      "\u001b[1m4779/4779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9175 - loss: 0.2239 - val_accuracy: 0.9203 - val_loss: 0.2264\n",
      "Epoch 19/50\n",
      "\u001b[1m4779/4779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9204 - loss: 0.2194 - val_accuracy: 0.9243 - val_loss: 0.2085\n",
      "Epoch 20/50\n",
      "\u001b[1m4779/4779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9199 - loss: 0.2190 - val_accuracy: 0.9237 - val_loss: 0.2232\n",
      "Epoch 21/50\n",
      "\u001b[1m4779/4779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9206 - loss: 0.2172 - val_accuracy: 0.9238 - val_loss: 0.2255\n",
      "Epoch 22/50\n",
      "\u001b[1m4779/4779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9218 - loss: 0.2155 - val_accuracy: 0.9249 - val_loss: 0.2256\n",
      "Epoch 23/50\n",
      "\u001b[1m4779/4779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9227 - loss: 0.2152 - val_accuracy: 0.9296 - val_loss: 0.2104\n",
      "Epoch 24/50\n",
      "\u001b[1m4779/4779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9226 - loss: 0.2113 - val_accuracy: 0.9203 - val_loss: 0.2199\n",
      "\u001b[1m612/612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 92.43%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.88      0.93      9558\n",
      "           1       0.80      0.89      0.84      1145\n",
      "           2       0.72      0.97      0.83      2302\n",
      "           3       0.97      0.98      0.98      6557\n",
      "\n",
      "    accuracy                           0.92     19562\n",
      "   macro avg       0.87      0.93      0.89     19562\n",
      "weighted avg       0.94      0.92      0.93     19562\n",
      "\n",
      "LSTM model saved as lstm_alarm_model.h5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Load processed train-test datasets\n",
    "X_train = np.load(\"X_train.npy\")\n",
    "y_train = np.load(\"y_train.npy\")\n",
    "X_test = np.load(\"X_test.npy\")\n",
    "y_test = np.load(\"y_test.npy\")\n",
    "\n",
    "# Reshape for LSTM (samples, timesteps, features)\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(64, activation='relu', return_sequences=True, input_shape=(1, X_train.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    BatchNormalization(),\n",
    "    LSTM(32, activation='relu', return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(4, activation='softmax')  # 4 classes for severity levels\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model with Early Stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print evaluation results\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Save model\n",
    "model.save(\"lstm_alarm_model.h5\")\n",
    "print(\"LSTM model saved as lstm_alarm_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bfb8b33-6fae-4b74-8e13-372137baef3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 616 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7fb014369630> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 616 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7fb014369630> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step\n",
      "Real-time Predictions (Filtered):\n",
      "             Timestamp  Predicted Severity  Alarm ID\n",
      "0  2025-02-26 13:09:14                   1       600\n",
      "1  2025-02-26 13:09:14                   3       850\n",
      "2  2025-02-26 13:09:14                   1       126\n",
      "3  2025-02-26 13:09:14                   1       721\n",
      "4  2025-02-26 13:09:14                   1       559\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "\n",
    "# Load trained model\n",
    "model = tf.keras.models.load_model(\"lstm_alarm_model.h5\")\n",
    "\n",
    "# Cek jumlah fitur yang digunakan saat training\n",
    "df = pd.read_csv(\"Cleaned_Merged_data_alarm.csv\")\n",
    "num_features = df.drop(columns=[\"Severity\"]).shape[1]  # Ambil jumlah fitur yang benar\n",
    "\n",
    "# Generate dummy real-time data dengan jumlah fitur yang benar\n",
    "num_samples = 10  # Jumlah sampel baru\n",
    "dummy_data = np.random.rand(num_samples, num_features)  # Data acak antara 0-1\n",
    "\n",
    "# Normalisasi dengan StandardScaler (gunakan scaler yang sama dari training)\n",
    "scaler = StandardScaler()\n",
    "dummy_data_scaled = scaler.fit_transform(dummy_data)  \n",
    "\n",
    "# Reshape untuk LSTM\n",
    "X_real_time = dummy_data_scaled.reshape((num_samples, 1, num_features))\n",
    "\n",
    "# Predict\n",
    "y_real_time_pred = np.argmax(model.predict(X_real_time), axis=1)\n",
    "\n",
    "# Generate real-time timestamp\n",
    "timestamps = [datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") for _ in range(num_samples)]\n",
    "\n",
    "# Generate dummy Alarm ID (random values between 1-1000)\n",
    "alarm_ids = np.random.randint(1, 1000, size=num_samples)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_real_time = pd.DataFrame({\n",
    "    \"Timestamp\": timestamps,\n",
    "    \"Predicted Severity\": y_real_time_pred,\n",
    "    \"Alarm ID\": alarm_ids\n",
    "})\n",
    "\n",
    "\n",
    "# Print hasil prediksi\n",
    "print(\"Real-time Predictions (Filtered):\")\n",
    "print(df_real_time.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2c51f10-1b90-427c-a20d-8fd0f83109ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step\n",
      "Predicted Future Alarm:\n",
      "             Timestamp  Predicted Severity  Alarm ID\n",
      "0  2025-02-26 14:11:22                   3        22\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load trained model\n",
    "model = tf.keras.models.load_model(\"lstm_alarm_model.h5\")\n",
    "\n",
    "# Cek jumlah fitur yang digunakan saat training\n",
    "df = pd.read_csv(\"Cleaned_Merged_data_alarm.csv\")\n",
    "num_features = df.drop(columns=[\"Severity\"]).shape[1]  # Ambil jumlah fitur yang benar\n",
    "\n",
    "# Load real-time data terbaru\n",
    "latest_data = np.random.rand(1, num_features)  # Dummy real-time data\n",
    "\n",
    "# Normalisasi dengan StandardScaler (gunakan scaler yang sama dari training)\n",
    "scaler = StandardScaler()\n",
    "latest_data_scaled = scaler.fit_transform(latest_data)  \n",
    "\n",
    "# Reshape untuk LSTM\n",
    "X_future = latest_data_scaled.reshape((1, 1, num_features))\n",
    "\n",
    "# Predict severity di masa depan\n",
    "y_future_pred = np.argmax(model.predict(X_future), axis=1)[0]\n",
    "\n",
    "# Prediksi waktu di masa depan (misalnya 1 jam ke depan)\n",
    "future_timestamp = (datetime.now() + timedelta(hours=1)).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Generate dummy Alarm ID untuk prediksi\n",
    "future_alarm_id = np.random.randint(1, 1000)\n",
    "\n",
    "# Buat DataFrame untuk hasil prediksi\n",
    "df_future = pd.DataFrame({\n",
    "    \"Timestamp\": [future_timestamp],\n",
    "    \"Predicted Severity\": [y_future_pred],\n",
    "    \"Alarm ID\": [future_alarm_id]\n",
    "})\n",
    "\n",
    "\n",
    "# Print hasil prediksi\n",
    "print(\"Predicted Future Alarm:\")\n",
    "print(df_future)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe069a4d-e84a-493c-a99c-2b99f8222faf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
